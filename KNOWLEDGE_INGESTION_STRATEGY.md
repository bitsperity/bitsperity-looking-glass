# Knowledge Ingestion Strategy

**Version:** 1.0  
**Datum:** 2025-10-22

## ðŸŽ¯ Philosophie: Quality over Quantity

Ariadne ist ein **Knowledge Graph fÃ¼r strukturiertes, validiertes Wissen**. Wir priorisieren **Precision** und **QualitÃ¤t** Ã¼ber automatisierte Bulk-Ingestion mit hohem Noise.

---

## âŒ Was wir NICHT tun

### Automatische News Ingestion

**FrÃ¼here Implementierung (entfernt):**
- spaCy NER: Einfache Named Entity Recognition
- Pattern-Matching: Regel-basierte Relation Extraction
- Automatisches HinzufÃ¼gen von News â†’ Entities â†’ Relations

**Warum entfernt?**

1. **Niedrige Precision:**
   - spaCy erkennt "Apple" als Company, aber nicht "Fed" als Concept
   - Keine Disambiguierung (Apple Inc. vs. apple fruit)
   - Keine Ticker-Resolution ohne manuelle Lookup-Tabellen

2. **Schwache Relation Extraction:**
   - Pattern-Matching: "X supplies Y" âœ… erkannt
   - Aber: "NVIDIA relies heavily on TSMC" âŒ nicht erkannt
   - Keine KausalitÃ¤t, Kontext oder Negation

3. **Hoher Noise:**
   - Irrelevante Entities (False Positives)
   - Unstrukturierte Facts ohne Confidence-Berechnung
   - Keine Validierung

4. **Fehlende Temporal Logic:**
   - Wann gilt ein Fakt? (valid_from/valid_to)
   - Alte Fakten Ã¼berschreiben oder versionieren?

**Ergebnis:** Graph wird mit unzuverlÃ¤ssigen Daten gefÃ¼llt â†’ schadet mehr als es hilft.

---

## âœ… Was wir stattdessen tun

### LLM-Agent-gesteuerter Graph-Aufbau

**Prinzip:** Agent liest News (via Tesseract Semantic Search), versteht Kontext mit LLM-Brain, und schreibt **strukturiert** in Ariadne.

### Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Agent Daily Routine: News Discovery                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Tesseract Semantic Search               â”‚
        â”‚ Query: "AI regulation earnings OPEC"    â”‚
        â”‚ â†’ Top-10 relevante News                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Agent reads & analyzes mit LLM                             â”‚
â”‚                                                                â”‚
â”‚ News: "NVDA Q3 Earnings Beat: Datacenter Revenue +94% YoY"    â”‚
â”‚                                                                â”‚
â”‚ Agent Thought:                                                 â”‚
â”‚ - "NVDA mentioned â†’ positive Event"                            â”‚
â”‚ - "Datacenter strength â†’ supports Hyperscaler CapEx thesis"   â”‚
â”‚ - "This is EVIDENCE for hypothesis: NVDA benefits from AI"    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Agent schreibt strukturiert in Ariadne                     â”‚
â”‚                                                                â”‚
â”‚ Option A: High Confidence â†’ Fakt                              â”‚
â”‚   POST /v1/kg/fact                                            â”‚
â”‚   {                                                            â”‚
â”‚     "source_label": "Event",                                  â”‚
â”‚     "source_id": "event-nvda-earnings-q3",                    â”‚
â”‚     "target_label": "Company",                                â”‚
â”‚     "target_id": "nvda-node-id",                              â”‚
â”‚     "rel_type": "POSITIVE_IMPACT",                            â”‚
â”‚     "confidence": 0.95,                                       â”‚
â”‚     "valid_from": "2025-11-20T00:00:00"                       â”‚
â”‚   }                                                            â”‚
â”‚                                                                â”‚
â”‚ Option B: Unsicher â†’ Hypothese + Evidence sammeln             â”‚
â”‚   POST /v1/kg/hypothesis                                      â”‚
â”‚   POST /v1/kg/validate/hypothesis/{id}/evidence               â”‚
â”‚   ... nach N Evidence â†’ validate                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”§ LLM-Prompt fÃ¼r Strukturierte Extraction

### Beispiel: Agent analysiert News

**Prompt:**
```
Du bist ein Financial Intelligence Agent. Analysiere folgende News und extrahiere strukturierte Fakten fÃ¼r einen Knowledge Graph.

News:
---
Title: "Taiwan Semi to Expand Arizona Fab with $40B Investment"
Body: "TSMC announced plans to invest $40 billion in its Arizona semiconductor facility, marking the largest foreign investment in U.S. manufacturing history. The expansion will triple production capacity by 2026..."
---

Extrahiere:
1. **Entities:** Companies, Locations, Concepts
2. **Events:** Was ist passiert? (Name, Kategorie, Datum)
3. **Relations:** Welche Beziehungen existieren? (Source â†’ Target, Type, Confidence)
4. **Temporal Info:** Wann gilt dieser Fakt? (valid_from, valid_to)

Output als JSON:
{
  "entities": [
    {"id": "tsmc-001", "label": "Company", "properties": {"ticker": "TSM", "name": "Taiwan Semiconductor"}},
    {"id": "arizona-001", "label": "Location", "properties": {"name": "Arizona", "country": "USA"}},
    {"id": "semis-001", "label": "Concept", "properties": {"name": "Semiconductor Manufacturing", "category": "Industry"}}
  ],
  "events": [
    {
      "id": "event-tsmc-expansion-2025",
      "name": "TSMC Arizona Fab Expansion",
      "category": "CapEx",
      "occurred_at": "2025-10-20T00:00:00",
      "confidence": 0.95
    }
  ],
  "relations": [
    {
      "source_id": "tsmc-001",
      "target_id": "arizona-001",
      "rel_type": "EXPANDS_TO",
      "confidence": 0.95,
      "valid_from": "2025-10-20T00:00:00",
      "valid_to": null,
      "evidence": "$40B investment announced"
    },
    {
      "source_id": "event-tsmc-expansion-2025",
      "target_id": "tsmc-001",
      "rel_type": "AFFECTS",
      "confidence": 0.90,
      "valid_from": "2025-10-20T00:00:00"
    }
  ]
}
```

**Agent verwendet Output fÃ¼r:**
- `POST /v1/kg/fact` (fÃ¼r jede Relation)
- Oder: `POST /v1/kg/hypothesis` (falls unsicher) + spÃ¤ter Evidence sammeln

---

## ðŸ¤– Automatisierung wo sinnvoll: Price Events

### Was automatisiert werden kann

**Technische Analyse ist deterministisch:**
- MA-Crossover (Golden Cross / Death Cross)
- Breakouts (52-week high/low)
- Volatility Spikes (ATR > 2Ïƒ)
- RSI Extreme Levels (>70 / <30)
- Volume Anomalies

**Warum das OK ist:**
- **Mathematisch definiert** (keine AmbiguitÃ¤t)
- **Keine Interpretation nÃ¶tig** (RSI > 70 = overbought, objektiv)
- **Hohe Relevanz** fÃ¼r Timeline-Context und Pattern-Correlation

### Endpoint: `/v1/kg/ingest/prices`

**Implementiert:**
```python
PriceEventDetector.detect_all(symbol, price_data)
â†’ Returns: [
    {
        "event_type": "ma_crossover",
        "occurred_at": "2025-09-20T00:00:00",
        "confidence": 1.0,  # Deterministisch
        "properties": {
            "ma_short": 50,
            "ma_long": 200,
            "direction": "bullish"
        }
    }
]
```

**Graph Write:**
- `(PriceEvent:Node)` mit `event_type`, `occurred_at`, `properties`
- `(Company)-[HAS_PRICE_EVENT]->(PriceEvent)`

**Use Cases:**
- Timeline: "Welche PriceEvents gab es bei NVDA im September?"
- Pattern-Correlation: "MA-Crossover + Fed-Event = Trading Signal?"
- Regime Detection: "High Volatility Period" via ATR-Spikes

---

## ðŸ“Š Quality Metrics

### Wie messen wir Graph-QualitÃ¤t?

**Metriken:**
1. **Fact Confidence:** Durchschnittliche Confidence aller Edges (Ziel: >0.80)
2. **Hypothesis Validation Rate:** % validierter Hypothesen (vs. invalidiert)
3. **Temporal Coverage:** % Edges mit `valid_from`/`valid_to` (Ziel: 100%)
4. **Version Consistency:** Keine Duplikate ohne Version-Tracking
5. **Evidence Ratio:** Hypothesen mit â‰¥3 Evidence vor Validation

**Dashboard (zukÃ¼nftig):**
```
Graph Stats:
- Total Nodes: 1,234
- Total Edges: 5,678
- Avg Confidence: 0.87 âœ…
- Temporal Edges: 98% âœ…
- Pending Hypotheses: 12
- Validated Patterns: 8
```

---

## ðŸ”„ Hybrid-Ansatz (ZukÃ¼nftig)

### LLM + Rules (Best of Both Worlds)

**Stufe 1: Rules fÃ¼r "Easy Cases"**
- "TSMC supplies to NVIDIA" â†’ `SUPPLIES_TO` (regex)
- "Apple competes with Samsung" â†’ `COMPETES_WITH` (pattern)
- Confidence: 0.70 (niedrig, da rule-based)

**Stufe 2: LLM fÃ¼r "Hard Cases"**
- Komplexe SÃ¤tze, KausalitÃ¤t, Negation
- "While NVIDIA benefits from AI demand, supply chain constraints remain a risk" â†’ 2 Relations:
  - `(AI Demand)-[BENEFITS]->(NVIDIA)` [confidence: 0.85]
  - `(Supply Chain)-[RISK_FOR]->(NVIDIA)` [confidence: 0.75]

**Stufe 3: Human-in-the-Loop**
- Kritische Fakten (Confidence < 0.80) â†’ Agent fragt User
- "Unsicher: Soll ich 'Fed Rate Hike â†’ Tech Selloff' als Fakt speichern?"
- User bestÃ¤tigt â†’ Confidence = 0.95

---

## ðŸ“ Ariadne Write Endpoints (Manual Building)

### Endpoint-Ãœbersicht

| Endpoint | Use Case | Wer nutzt? |
|----------|----------|------------|
| `POST /v1/kg/fact` | Validierter Fakt mit Temporal Bounds | Agent (nach LLM-Analysis) |
| `POST /v1/kg/observation` | Unstrukturierte Beobachtung | Agent (Daily Notes) |
| `POST /v1/kg/hypothesis` | Vermutung, die validiert werden muss | Agent (Research) |
| `POST /v1/kg/validate/.../evidence` | Evidence sammeln | Agent (liest weitere News) |
| `POST /v1/kg/validate/.../validate` | Finale Validation â†’ Pattern | Agent (nach Threshold) |

---

## ðŸš€ Roadmap: Verbesserungen

### Kurzfristig (MVP)
- âœ… Manual Graph Building via Write Endpoints
- âœ… Price Events Automation
- âœ… Hypothesis Validation Workflow
- â³ Agent Prompt Library (standardisierte LLM-Prompts)

### Mittelfristig
- ðŸ”„ Fine-tuned NER Model (FinBERT fÃ¼r Financial Entities)
- ðŸ”„ LLM-based Relation Extraction (GPT-4 Structured Output)
- ðŸ”„ Confidence Calibration (ML-Model fÃ¼r Confidence-Score)

### Langfristig
- ðŸ”® Graph-RAG Integration (Microsoft GraphRAG)
- ðŸ”® Multi-Agent Validation (Agent A erstellt, Agent B validiert)
- ðŸ”® Auto-Deduplication (Entity Resolution via Embeddings)

---

## ðŸ“– Best Practices fÃ¼r Agents

### DO âœ…

1. **Immer `valid_from` setzen** (auch bei aktuellen Fakten)
2. **Confidence realistisch schÃ¤tzen:**
   - LLM-extracted: 0.70-0.85
   - Human-verified: 0.90-0.95
   - Deterministisch (Math): 1.0
3. **Unsichere Fakten â†’ Hypothesen** (nicht direkt als Fakt)
4. **Evidence sammeln** (min. 3x) vor Validation
5. **Temporal Updates:** Alte Edge mit `valid_to` schlieÃŸen, neue Edge anlegen

### DON'T âŒ

1. âŒ Keine Bulk-Imports ohne LLM-Review
2. âŒ Keine Fakten ohne `valid_from`
3. âŒ Keine "guessed" Confidence-Werte (0.5 = "no idea")
4. âŒ Keine Duplikate ohne Version-Tracking
5. âŒ Keine Auto-Ingestion von unverifizierten News

---

## ðŸ“š Referenzen

- **Ariadne API Reference:** `/ARIADNE_API_REFERENCE.md`
- **Architecture:** `/kg_and_memory.md`
- **Implementation Plan:** `/plan.md`
- **Price Event Detectors:** `/libs/ariadne_core/signals/price_detectors.py`

---

**Fazit:** Manuelle, LLM-gesteuerte Graph-Pflege > automatisierte Bulk-Ingestion. Quality over Quantity. Ariadne ist ein **Precision Tool**, kein Data Lake.

